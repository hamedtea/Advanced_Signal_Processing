%% Part 1: Generation of an AR(1) processs
clear all; close all
%% AR(1) with correlation coefficients a = 0.99
a = 0.9;
% Generate the AR(1) process and show the input and output of the model
for irealiz = 1:10
    N = 20000;
    e  = randn(N,1); % white noise of zero mean
    y1 = 0.;
    for ii = 2:N
        y1(ii) = a * y1(ii-1) + e(ii); % generate the AR(1) process
    end
    figure(1)
    subplot(211)
    plot(1:N,e(1:N))
    title('Input: white noise','Fontsize',20)
    xlabel('time t','Fontsize',16)
    ylabel('e_t','Fontsize',16)
    subplot(212)
    plot(1:N,y1)
    title(['Output: AR(1), a = ' num2str(a)] ,'Fontsize',20)
    xlabel('time t','Fontsize',16)
    ylabel('y_t','Fontsize',16)
    mean_std_max(irealiz,1:7) = [mean(y1(1:700)), std(y1(1:700)) max(abs(y1(1:700))) ...
                                   mean(y1(10000:end)), std(y1(10000:end)) max(abs(y1(10000:end))) max(abs(y1(10000:end)))/std(y1(10000:end))];
end
mean_std_max
% Background hints:
% (1) a^50 = 0.60; a^450 = 0.01; a^680 = 0.001; a^2000 = 1.8e-9;a^10000=2.2e-44;
% (2) r_0 = sigma_e^2/(1-a^2) is the variance for stationary regim and is called here true autocorrelation
% (3) for e generated by the function randn sigma_e^2 = 1
% (4) Repeating the generation process 10 times we get some statistics
% collected in the matrix mean_std_max.

%% Q1: AR(1) transitory and stationary parts
% Question:
% Which of the following statements are true? (there might be multiple correct statements)
%
% a) The input e and the output y are generated in such a way that their expectations
% are as follows: Ee = 0 and Ey are different of 0.
% b) We expect a transitory regim of less than 1000 samples, after which the
% process can be approximated to be stationary.
% c) The range of the output y is about (-25,25), because the variance sigma_y^2 = Ey_i^2 is
% 1/(1-a^2)=  50.251, the standard deviation is about sigma_y = 7.1, and hence we expect
% most of the time the signal to be within (-4*sigma_y,+4*sigma_y).
% d)  The estimated standard deviation for the transitory part is similar to the 
% standard deviation for the stationary part.
% e) the maximum value over the transitory part is noticeably smaller than for 
% the stationary part, because the number of random points in the transitory
% parts is smaller than the number points considered for the permanent regim

%% Part 2: Estimate the auto-correlation function of an AR(1) processs
%% Experiment with three cases for the generated data length

N1  = [5*10^5 10^5 10^4];%  three values for the length of data
% a = 0.99  is high, but not extremely close to 1
a  = 0.99; % a is also called the correlation coefficient

max_tau = 2000; % we estimate r(0),r(1),...,r(max_tau)

% The analytic expression of the autocorrelation function
r_true(1+(0:max_tau)) = (a.^(0:max_tau))/(1-a^2);

colors  = ['r-';'g-';'c-';'b-']
figure(2), clf,plot(0:max_tau,r_true,'b-'), hold on

for len = 1:3 
    N = N1(len);        % Maximum value of discrete time
    % Nt = floor(N/4);  % The time series is cropped from Nt,Nt+1,..., N
    Nt = 2*10^4/4;      % Let's neglect the same initial segment, 1...Nt, for all three cases
    e  = randn(N,1);
    y1 = 0.;
    
    for ii = 2:N
        y1(ii) = a*y1(ii-1) + e(ii); % generate the AR(1) process
    end
    y = y1(Nt:N);
    Ny = length(y);
    %
    % Estimate the autocorrelation function from data, knowing that Ey = 0
    %
    r = zeros(max_tau+1,1);
    for tau = 0:max_tau
        if(rem(tau,100)==0),[tau max_tau],end % show the iteration progressing
        time_segment = 1:(Ny-tau);
        r(tau+1) = sum(y(time_segment).* y(tau+time_segment))/(Ny-tau);
    end
    %
    plot(0:max_tau,r,colors(len,:))
    if(len==2)
        r_estim = r;
    end
end
title(['Autocorrelation function, a = ' num2str(a)],'Fontsize',14)
xlabel('lag \tau','Fontsize',12)
ylabel('r(\tau)','Fontsize',12)
legend('true r','N=5*10^5','N=10^5','N=10^4')  

%% Q2
% Background
% We consider here the following helpful results:
% (1) E[y_n*y_{n+k}] = a^k*(1-a^{2n+2})/(1-a^2)
% (2) In the stationary regim r_k = a^k/(1-a^2), which  is called here true
%       autocorrelation
%
% Question 2: State which of the following statements are true:
% a) For the first 50 correlation values the estimates are  reasonably close to the
% true value, for all three cases of N
% b) for the case N = 10^4 the estimates are not reliable, because there are not enough data points 
% c) The efect of non-stationarity shown in Ey_ny_{n+k} =
% a^k*(1-a^{2n+2})/(1-a^2) makes the estimates at line 84 unreliable, for all values of
% numbers of data, N, and the given value of Nt at line 68

%% Part 3: Discuss the effect of accuracy of estimating the autocorrelation 
%% function for the adaptive noise cancellation (ANC) application
% In the ANC, the Wiener-Hopf optimal filter design requires the inversion
% of the correlation matrix. The needed order of the filter might be quite
% large for ANC (from hudreads to thousands, as you estimated it in Lecture 1,
% bonus assignment)
%
% Assume that in ANC the source of noise is the AR(1) process, from Part 1
% and the estimated autocorrelation was done as in Part 2, for N = 10^5
% Consider a filter of order 8 but also experiment with order 200 to check
% if the results are generalizing

%% Case 1: consider the true autocorrelation function r_true
% The autocorrelation matrix  R needed in Wiener-Hopf design w = R^(-1)p

R = toeplitz(r_true(1:200));
figure(3),imagesc(R), colormap(gray),title('Autocorrelation matrix R')
invR = inv(R);
figure(4),imagesc(invR), colormap(gray),title('Inverse of autocorrelation matrix R'),colorbar


%% Case 2: consider the estimated autocorrelation as in Part 2 using N = 10^5 
% The autocorrelation matrix  R needed in Wiener-Hopf design w = R^(-1)p

Restim = toeplitz(r_estim(1:200));
figure(5),imagesc(R), colormap(gray),title('Estimated autocorrelation matrix R')
invRestim = inv(Restim);
figure(6),imagesc(invRestim), colormap(gray),title('Inverse of estimated autocorrelation matrix R'),colorbar

%% Questions Q3:
%
% Q3: Inspect the matrix inv(R). It is a band matrix. Which statements are
% true?
% a) inv(R) has only three  nonzero diagonals: the main, the first upper, and the first lower
% diagonals. 
% b) the first upper diagonal and the first lower diagonal are equal to -a
% c) the main diagonal has all the elements equal to 1+a^2, except the first and last elements, which are 1.
% d) the elements on the second main diagonal should be zero, but they are nonzero due to computational errors when computing inv(R) in matlab. 
% e) If one needs the inverse of the autocorrelation matrix, for example in
% Wiener-Hopf optimal design, and one knows that the input signal to the filter is AR(1),
% one needs only to estimate the parameter a, and then 
% inv(R) can be obtained by setting the bands correctly according to the estimate of a
% f) All the statements found true for order 8 remain true for order 200




%% Part 4: LS estimate of parameters for AR(6)

% Choose a ground truth model AR(6)
% The roots of the AR polynomial are 
j = sqrt(-1);
z=[];
z(1)= 0.95*exp(j*pi/3);
z(2)= 0.95*exp(-j*pi/3);
z(3)= 0.85*exp(j*(pi/2+pi/3));
z(4)= 0.85*exp(-j*(pi/2+pi/3));
z(5)= 0.85*exp(j*(2*pi/3));
z(6)= 0.85*exp(-j*(2*pi/3));
A = poly(z);
figure(11),clf, plot(real(z),imag(z),'bo')
title('Positions of the roots of A(z)')
axis equal
grid on
r1 = 1;
t = linspace(0,2*pi,100);
hold on
plot(r1*cos(t),r1*sin(t),'r-')
% Generate data with the AR polynomial

N = 1000;
Nt = floor(N/4);   % The time series is cropped from Nt,Nt+1,..., N
e  = randn(N,1);
y1 = zeros(N,1);

for ii = 7:N
    y1(ii) = -A(2:7)*y1((ii-1):-1:(ii-6)) + e(ii); % generate the AR(1) process
end
y = y1(Nt:end);
Ny = length(y);
%
% Estimate the autocorrelation function from data, knowing that Ey = 0
%
max_tau = 20;
r = zeros(max_tau+1,1);
for tau = 0:max_tau
    %if(rem(tau,100)==0),[tau max_tau],end
    time_segment = 1:(Ny-tau);
    r(tau+1) = sum(y(time_segment).* y(tau+time_segment))/(Ny-tau);
end
% LS estimate of the AR(M) model
% y(n) = a1*y(n-1)+
Mmax = 15;
thetaAll = zeros(Mmax);
for M = 1:Mmax
    % Construct the normal system matrix and free vector
    Phi = zeros(M,M); Psi = zeros(M,1);
    for n = (Mmax+1):Ny
        Phi = Phi + y((n-1):-1:(n-M))*y((n-1):-1:(n-M))';
        Psi = Psi + y((n-1):-1:(n-M))*y(n);
    end
    % Find the LS parameters
    theta = Phi\Psi;
    hat_e = zeros(Ny,1);
    for n = (Mmax+1):Ny
        hat_e(n) = y(n) - theta'*y((n-1):-1:(n-M));
    end
    J(M) = mean(hat_e.^2);
    thetaAll(1:M,M) = theta;
end
thetaAll
J
A
figure(12),plot(J,'or-'),grid on
xlabel('Order M')
ylabel('Criterion J(M)')

Aestim = [1; -thetaAll(1:6,6)]
[roots(A(:)) roots(Aestim(:))]
zest = roots(Aestim(:))


%% Questions Q4: Accuracy of LS estimates. What is true?
% a) Compare the LS parameter thetaAll(:,6) with the AR  polynomial given
% in A. Is it true that from the LS estimates one can reconstruct a good approximation of the
% polynomial A?
% b) By plotting on the unit circle the roots zest of the polynomial obtained after LS estimation, they are very far from
% the roots of the original polynomial A(z).
%
%% Q5: Determine visually from Figure 12 what is the true order of the AR(M^*) process:
% a) M^* = 4
% b) M^* = 6
% c) M^* = 5
        
%% Part 5: Three structure selection criteria: FPE against AIC against MDL

% FPE
% Consider FPE = ((N+k)/(N-k))*sigma2
% log FPE = log((N+k)/(N-k)) + log(sigma2)
% AIC
% AIC = N*log(sigma2) + 2*k
% AIC/N = log(sigma2) + 2*k/N
% MDL = N*log(sigma2) + k*log(N)
% MDL/N = log(sigma2) + k*log(N)/N

% Penalty in log(FPE) is log((N+k)/(N-k)) = log((1+k/N)/(1-k/N)) \approx
% (k/N)-(-k/N) = 2*k/N
% Penalty in AIC/N  is 2*k/N
% Task: plot the penalty terms in both expressions and compare
N = 101;
k = 1:(N-1);
figure(20),plot(k,log((N+k)./(N-k)),'or-',k, 2*k/N,'b-', k, k*log(N)/N,'g-'),xlabel('k'),ylabel('Penalty term')
legend('Penalty in log(FPE)', 'Penalty in AIC/N', 'Penalty in MDL/N'),grid on

%% Question Q6:
%
% Q6: Which of the following statements are true?
% a) Out of all three criteria, MDL criterion penalizes the most the increase of k, so it will decide smaller orders than the others
% b) Out of all three criteria, FPE criterion penalizes the most the increase of k, so it will decide smaller orders than the others criteria

%% Use the three structure selection criteria for the AR(1),...,AR(15) models obtained  in Part 4
Crit1 = log(J) + log((Ny+(1:15))./(Ny-(1:15)));
Crit2 = log(J) + 2*(1:15)/Ny;
Crit3 = log(J) + (1:15)*log(Ny)/Ny;
figure(21), plot( 1:15, Crit1,'vr-', 1:15, Crit2,'og-', 1:15, Crit3, 'bx-')
legend('FPE', 'AIC/N', 'MDL/N'),grid on
format shortg
crit = [Crit1'  Crit2' Crit3'];
[rol col] = find(abs(crit) == min(crit))
%% Q7: Consider the values of Crit1, Crti 2, and Crit 3. Each criterion is 
% used for picking the best model order. In this case, which of the methods 
% selects the correct model order?
% a)FPE; b) AIC; c)MDL (multiple possible correct answers)
%
 
